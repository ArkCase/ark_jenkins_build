#!/bin/bash
SCRIPT="$(readlink -f "${BASH_SOURCE:-${0}}")"
BASEDIR="$(dirname "${SCRIPT}")"

set -euo pipefail
. /.functions

set_or_default DEPLOY_TIMEOUT "20m"

set_as_boolean DISABLE_DEPLOY
export DISABLE_DEPLOY

set_as_boolean PARTIAL_DEPLOYMENT
export PARTIAL_DEPLOYMENT

execute()
{
	execute_unless DISABLE_DEPLOY "${@}"
	return ${?}
}

ensure_namespace_exists()
{
	local NAMESPACE="${1}"
	execute kubectl get namespace "${NAMESPACE}" &>/dev/null && return 0
	execute kubectl create namespace "${NAMESPACE}" && return 0
	return 1
}

sanitize_paths()
{
	local STR="${1}"
	local SEP=":"
	local PARTS=()

	# First, split into parts so we can retain the
	# declaration order
	readarray -t PARTS < <(tr "${SEP}" "\n" <<< "${STR}" | sed -e '/^\s*$/d')
	local LIST=":"
	for P in "${PARTS[@]}" ; do
		[ -n "${P}" ] || continue
		# Skip it if already marked
		fgrep -q ":${P}:" <<< "${LIST}" && continue
		LIST+="${P}:"
		echo "${P}"
	done
}

sanitize_common_dirs()
{
	local COMMON_DIRS=()
	# We split this in two so it's easy to lift the sanitize_paths function for other purposes
	readarray -t COMMON_DIRS < <(sanitize_paths "${1}")
	for C in "${COMMON_DIRS[@]}" ; do
		[[ "${C}" =~ ^(/*)(.*)$ ]] && C="${BASH_REMATCH[2]}"
		if [[ "${C}" =~ (^|/)[.]{1,2}(/|$) ]] ; then
			warn "Skipping the common directory specification [${C}]: may not contain '.' or '..' as path components"
			continue
		fi
		echo "${C}"
	done
}

get_artifacts_sum()
{
	local RELEASE="${1}"
	local SELECTOR="app.kubernetes.io/instance=${RELEASE},app.kubernetes.io/name=app,app.kubernetes.io/part=artifacts"
	local POD=""

	#
	# We try to run the sum on all existing pods
	#
	while read POD ; do
		# Run the sums. We do this in case networking is not
		# available to us. We know kubectl *is*, so use that!
		SUM="$(timeout 30 kubectl exec -it "${POD}" -- /usr/local/bin/global-sums 2>/dev/null)" || continue

		# The SUM must be valid JSON output
		[ -n "${SUM}" ] || continue
		jq -r <<< "${SUM}" &>/dev/null || continue

		# We're good to go, so return it!
		echo -en "${SUM}"
		break
	done < <(kubectl get pod --selector "${SELECTOR}" -o name | sort)
	return 0
}

list_pod_uids()
{
	local RELEASE="${1}"
	local SELECTOR="app.kubernetes.io/instance=${RELEASE},arkcase.com/deploys=true"
	kubectl get pod --selector "${SELECTOR}" -o json | jq -r '.items[].metadata | .uid + " " + .name'
}

list_changed_categories()
{
	local RELEASE="${1}"
	local OLD_ARTIFACTS="${2}"
	local NEW_ARTIFACTS="$(get_artifacts_sum "${RELEASE}")"

	# If the sums are the same, we simply do nothing
	[ "${OLD_ARTIFACTS}" == "${NEW_ARTIFACTS}" ] && return 0
	[ -n "${OLD_ARTIFACTS}" ] || OLD_ARTIFACTS="{}"
	[ -n "${NEW_ARTIFACTS}" ] || NEW_ARTIFACTS="{}"

	# If they're not, we need to be a bit more nuanced
	local OLD=""
	local NEW=""
	local CATEGORY=""
	while read CATEGORY ; do
		OLD="$(jq -r ".${CATEGORY}" <<< "${OLD_ARTIFACTS}")"
		NEW="$(jq -r ".${CATEGORY}" <<< "${NEW_ARTIFACTS}")"

		# If the artifact category no longer exists, then
		# we don't account for it as a changed category
		[ -n "${NEW}" ] || continue

		# This has changed (either didn't exist, or has a different
		# checksum), so we account for it as a changed category
		[ "${OLD}" != "${NEW}" ] && echo "${CATEGORY}"
	done < <(jq -r '.art[0] * .art[1] | keys[]' <<< "{ \"art\": [${OLD_ARTIFACTS}, ${NEW_ARTIFACTS}]}" | sort -u)
	return 0
}

list_changed_pods()
{
	local RELEASE="${1}"
	local OLD_PODS="${2}"
	shift 2
	local CATEGORIES=( "${@}" )

	local OLD=""
	local OLD_UID=""
	local NEW=""
	local NEW_UID=""
	for CATEGORY in "${CATEGORIES[@]}" ; do

		# Compare each pod's UID from before and now to identify which
		# pods were rolled already, vs. which ones still need rolling
		while read POD ; do
			read OLD_UID OLD < <(grep -e " ${POD}$" <<< "${OLD_PODS}") || true
			read NEW_UID NEW < <(grep -e " ${POD}$" <<< "${NEW_PODS}") || true

			# If the pod existed, and there's a change in the UID, then
			# the pod needs to be rolled, so add it to the list!
			[ -n "${OLD_UID}" ] && [ "${OLD_UID}" != "${NEW_UID}" ] && echo -e "${POD}"
		done < <(kubectl get pods --selector "arkcase.com/deploy-${CATEGORY}=true" -o name | sed -e 's;^pod/;;g')
	done | sort -u
	return 0
}

roll_pods_if_necessary()
{
	local RELEASE="${1}"
	local OLD_ARTIFACTS="${2}"
	local OLD_PODS="${3}"

	# Get the list of changed categories
	local CATEGORIES=()
	readarray -t CATEGORIES < <(list_changed_categories "${RELEASE}" "${OLD_ARTIFACTS}")

	# If no categories changed, we do nothing because any pods that would
	# need to be rolled due to template changes would have been rolled already...
	if [ ${#CATEGORIES[@]} -gt 0 ] ; then
		ok "No changes in artifact categories!"
		return 0
	fi

	# We have a list of changed categories, so roll their pods
	eyes "The following artifact categories have changed: [ ${CATEGORIES[@]} ]"

	eyes "Searching for pods that consume the changed artifact categories..."
	local NEW_PODS="$(list_pod_uids "${RELEASE}")"

	local ROLL=()
	readarray -t ROLL < <(list_changed_pods "${RELEASE}" "${OLD_PODS}" "${CATEGORIES[@]}")

	# rolled, we simply return
	if [ ${#ROLL[@]} -eq 0 ] ; then
		ok "All pods that needed to be rolled have already been rolled!"
		return 0
	fi

	# And ... roll!!
	doing "These pods need to be rolled due to deployment artifact changes: [ ${ROLL[@]} ]"
	if ! execute kubectl delete pod "${ROLL[@]}" --wait --timeout 5m ; then
		err "Pod roll timed out - the deployment may end up unstable..."
		return 1
	fi

	# Now we wait ...
	waiting "Wait for the rolled pods to become ready again..."
	if ! execute kubectl wait --for=condition=Ready pod "${ROLL[@]}" --timeout 5m ; then
		err "Failed to wait for the rolled pods to become initialized"
		return 1
	fi

	# All's well that ends well!
	ok "Rolled all pods dependent on the artifacts pod"
	return 0
}

usage()
{
	echo -e "usage: ${BASH_SOURCE:-${0}} [namespace] release chart environment [--helm extraHelmArgs...]" 1>&2
	exit 1
}

BASE_ARGS=()
for a in "${@}" ; do
	case "${a}" in
		--helm ) shift ; break ;;
		* ) BASE_ARGS+=("${a}") ; shift ;;
	esac
done
HELM_ARGS=("${@}")

set -- "${BASE_ARGS[@]}"

[ ${#} -ge 3 ] && [ ${#} -le 4 ] || usage

if [ "${#}" -eq 3 ] ; then
	NAMESPACE="$(kubectl config view --minify -o jsonpath="{..namespace}")"
	[ -n "${NAMESPACE}" ] || NAMESPACE="default"
else
	NAMESPACE="${1}"
	shift
fi
is_valid_hostname_part "${NAMESPACE}" || fail "Invalid namespace name [${NAMESPACE}]"

#
# The name of the helm release to deploy
#
RELEASE="${1}"
is_valid_hostname_part "${RELEASE}" || fail "Invalid release name [${RELEASE}]"

#
# The name of the helm chart to deploy
#
CHART="${2}"
[ -n "${CHART}" ] || fail "The chart name may not be the empty string"

#
# Ensure we got the right environment n ame
#
ENVIRONMENT_NAME="${3}"
is_valid_hostname_part "${ENVIRONMENT_NAME}" || fail "Invalid environment name [${ENVIRONMENT_NAME}]"

[ -v DEPLOYMENT_DIR ] || DEPLOYMENT_DIR=""
[ -n "${DEPLOYMENT_DIR}" ] || DEPLOYMENT_DIR="${PWD}"
DEPLOYMENT_DIR="$(readlink -f "${DEPLOYMENT_DIR}")" || fail "Failed to canonicalize the path for [${DEPLOYMENT_DIR}]"

say "Deploying from [${DEPLOYMENT_DIR}]..."
[ -f "${DEPLOYMENT_DIR}/deployment.yaml" ] || fail "This does not seem to be a deployment directory - no deployment.yaml was found!"

#
# This will hold the list of Helm "-f" parameters to include
# every single values file that needs to be taken into account
# for this deployment
#
VALUES=()

#
# Step one: find the common files
#
#
# The directory where the common files (i.e. files shared across
# directories) are stored.
#
[ -v COMMON_DIRS ] || COMMON_DIRS="common"
readarray -t COMMON_DIRS < <(sanitize_common_dirs "${COMMON_DIRS}")
for D in "${COMMON_DIRS[@]}" ; do
	COMMON_DIR="${DEPLOYMENT_DIR}/${D}"
	if [ -d "${COMMON_DIR}" ] ; then
		while read V ; do
			VALUES+=(-f "${V}")
		done < <(find "${COMMON_DIR}" -type f -iname '*.yaml' | sort -u)
	else
		warn "No common values are being applied from [${COMMON_DIR}] - the directory doesn't exist"
	fi
done

#
# Step two: find the files in the environment's directory
#
#
# The root directory where the environment directories
# are housed. The directory name must match the name
# of the environment being deployed *EXACTLY*
#
ENV_DIR="${DEPLOYMENT_DIR}/envs/${ENVIRONMENT_NAME}"

if [ -d "${ENV_DIR}" ] ; then
	while read V ; do
		VALUES+=(-f "${V}")
	done < <(find "${ENV_DIR}" -type f -iname '*.yaml' | sort -u)
else
	warn "No environment folder found at [${DEPLOYMENT_DIR}] for [${ENVIRONMENT_NAME}]"
fi

ensure_namespace_exists "${NAMESPACE}" || fail "Failed to ensure that the required namespace ${NAMESPACE} exists"

set -- "${HELM_ARGS[@]}"

OLD_ARTIFACTS="$(get_artifacts_sum "${RELEASE}")"
OLD_PODS="$(list_pod_uids "${RELEASE}")"

#
# First, try an in-place upgrade. If that fails, then undeploy and redeploy.
#
# Since we're using --wait, this will not exit until all the pods are ready,
# which allows us to analyze them safely and make a robust decision about
# which pods actually needed to be rolled, but weren't
#
RC=0
execute \
	helm upgrade --install \
		"${RELEASE}" "${CHART}" \
		--namespace "${NAMESPACE}" \
		--wait \
		--timeout "${DEPLOY_TIMEOUT}" \
		"${VALUES[@]}" \
		"${@}" || RC=${?}

if [ ${RC} -eq 0 ] ; then
	ok "Deployment complete!"
	roll_pods_if_necessary "${RELEASE}" "${OLD_ARTIFACTS}" "${OLD_PODS}" || err "Conditional pod rolling may have failed, tread carefully!"
	execute kubectl --namespace "${NAMESPACE}" get pods
	exit ${RC}
fi

err "In-place upgrade attempt failed (rc=${RC}), will try to uninstall and reinstall"

# The in-place upgrade failed ... we need to fully uninstall and
# then re-install the application!
#
# Propagate the "disable" flag, to do nothing if we're testing
DISABLE_UNDEPLOY="${DISABLE_DEPLOY}" "${BASEDIR}/undeploy-environment" "${NAMESPACE}" "${RELEASE}" || exit ${?}

# Execute the new installation, using install --replace
# because we intend to be preserving history from our
# uninstalls
#
# TODO: see above note re: Artemis rollout issues when clustering
# is enabled
RC=0
execute \
	helm install --replace \
		"${RELEASE}" "${CHART}" \
		--namespace "${NAMESPACE}" \
		--wait \
		--timeout "${DEPLOY_TIMEOUT}" \
		"${VALUES[@]}" \
		"${@}" || RC=${?}

if [ ${RC} -eq 0 ] ; then
	ok "Deployment complete!"
else
	# Something went wrong, try to offer up more information
	err "The Helm deployment has failed (rc=${RC}), please review the logs"
fi

execute kubectl --namespace "${NAMESPACE}" get pods
exit ${RC}
