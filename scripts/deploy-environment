#!/bin/bash
SCRIPT="$(readlink -f "${BASH_SOURCE:-${0}}")"
BASEDIR="$(dirname "${SCRIPT}")"

set -euo pipefail
. /.functions

set_or_default DEPLOY_TIMEOUT "20m"

set_as_boolean DISABLE_DEPLOY
export DISABLE_DEPLOY

set_as_boolean PARTIAL_DEPLOYMENT
export PARTIAL_DEPLOYMENT

set_as_boolean DISABLE_UPGRADE

set_or_default DEPLOY_LABEL "arkcase.com/deploys"

set_or_default HISTORY_MAX

execute()
{
	execute_unless DISABLE_DEPLOY "${@}"
	return ${?}
}

ensure_namespace_exists()
{
	local NAMESPACE="${1}"
	execute kubectl get namespace "${NAMESPACE}" &>/dev/null && return 0
	execute kubectl create namespace "${NAMESPACE}" && return 0
	return 1
}

sanitize_paths()
{
	local STR="${1}"
	local SEP=":"
	local PARTS=()

	# First, split into parts so we can retain the
	# declaration order
	readarray -t PARTS < <(tr "${SEP}" "\n" <<< "${STR}" | sed -e '/^\s*$/d')
	local LIST=":"
	for P in "${PARTS[@]}" ; do
		[ -n "${P}" ] || continue
		# Skip it if already marked
		fgrep -q ":${P}:" <<< "${LIST}" && continue
		LIST+="${P}:"
		echo "${P}"
	done
}

sanitize_common_dirs()
{
	local COMMON_DIRS=()
	# We split this in two so it's easy to lift the sanitize_paths function for other purposes
	readarray -t COMMON_DIRS < <(sanitize_paths "${1}")
	for C in "${COMMON_DIRS[@]}" ; do
		[[ "${C}" =~ ^(/*)(.*)$ ]] && C="${BASH_REMATCH[2]}"
		if [[ "${C}" =~ (^|/)[.]{1,2}(/|$) ]] ; then
			warn "Skipping the common directory specification [${C}]: may not contain '.' or '..' as path components"
			continue
		fi
		echo "${C}"
	done
}

roll_artifact_pods()
{
	local NAMESPACE="${1}"
	local RELEASE="${2}"
	local OLD_ARTIFACT_PODS="${3}"
	local EXTRA_SELECTOR="app.kubernetes.io/name=app,app.kubernetes.io/part=artifacts"
	local SELECTOR="app.kubernetes.io/instance=${RELEASE},${EXTRA_SELECTOR}"

	local NEW_ARTIFACT_PODS="$(list_artifact_uids "${NAMESPACE}" "${RELEASE}")"

	local RESOURCES="statefulset,deployment,replicaset,daemonset"

	# The deployment - in place or full - will have fully rolled all pods,
	# or not have rolled a single one. Thus: either these lists are identical
	# because no pods were rolled, or they're not in which case all pods were
	# rolled. As a result, we only have to act if the lists are identical.
	if [ "${OLD_ARTIFACT_PODS}" == "${NEW_ARTIFACT_PODS}" ] ; then
		local RC=0
		local OUT=""
		running "Rolling the artifacts pods to ensure any new artifacts take hold"
		OUT="$(kubectl --namespace "${NAMESPACE}" rollout restart "${RESOURCES}" --selector "${SELECTOR}" 2>&1)" || RC=${?}
		if [ ${RC} -ne 0 ] ; then
			err "Failed to roll the artifacts pod(s) (rc=${RC}):\n${OUT}"
			return 1
		fi

		sleeping "Waiting for the rolled artifacts pods to become ready again..."
		OUT="$(kubectl --namespace "${NAMESPACE}" rollout status "${RESOURCES}" --selector "${SELECTOR}" --timeout=5m 2>&1)" || RC=${?}
		if [ ${RC} -ne 0 ] ; then
			err "Timed out waiting for the rolled artifacts pod(s) to become ready (rc=${RC}):\n${OUT}"
			return 1
		fi
	fi

	ok "The artifact pods were rolled and are ready!"
	return 0
}

get_global_sums()
{
	local NAMESPACE="${1}"
	local RELEASE="${2}"
	local SELECTOR="app.kubernetes.io/instance=${RELEASE},app.kubernetes.io/name=app,app.kubernetes.io/part=artifacts"
	local POD=""

	#
	# We try to run the sum on all existing pods
	#
	while read POD ; do
		# First, wait for the pod to become ready ... up to 5 mins
		kubectl --namespace "${NAMESPACE}" wait --for condition=Ready "${POD}" --timeout=5m &>/dev/null || continue

		# Run the sums. We do this in case networking is not
		# available to us. We know kubectl *is*, so use that!
		SUM="$(timeout 30 kubectl --namespace "${NAMESPACE}" exec -it "${POD}" -- /usr/local/bin/global-sums 2>/dev/null)" || continue

		# The SUM must be valid JSON output
		[ -n "${SUM}" ] || continue
		jq -r <<< "${SUM}" &>/dev/null || continue

		# We're good to go, so return it!
		echo -en "${SUM}"
		return 0
	done < <(kubectl --namespace "${NAMESPACE}" get pod --selector "${SELECTOR}" -o name | sort)

	# If we weren't able to get the sum, we must notify as much
	return 1
}

list_pod_uids()
{
	local NAMESPACE="${1}"
	local RELEASE="${2}"
	local EXTRA_SELECTOR="${2:-}"
	[ -n "${EXTRA_SELECTOR}" ] && EXTRA_SELECTOR=",${EXTRA_SELECTOR}"
	local SELECTOR="app.kubernetes.io/instance=${RELEASE}${EXTRA_SELECTOR}"
	kubectl --namespace "${NAMESPACE}" get pod --selector "${SELECTOR}" -o json | jq -r '.items[].metadata | .uid + " " + .name'
}

list_artifact_uids()
{
	local NAMESPACE="${1}"
	local RELEASE="${2}"
	list_pod_uids "${NAMESPACE}" "${RELEASE}" "app.kubernetes.io/name=app,app.kubernetes.io/part=artifacts"
}

list_consumer_uids()
{
	local NAMESPACE="${1}"
	local RELEASE="${2}"
	list_pod_uids "${NAMESPACE}" "${RELEASE}" "${DEPLOY_LABEL}=true"
}

list_changed_categories()
{
	local OLD_ARTIFACTS="${1}"
	local NEW_ARTIFACTS="${2}"

	# If the sums are the same, we simply do nothing
	[ -n "${OLD_ARTIFACTS}" ] || OLD_ARTIFACTS="{}"
	[ -n "${NEW_ARTIFACTS}" ] || NEW_ARTIFACTS="{}"

	# If they're not, we need to be a bit more nuanced
	local OLD=""
	local NEW=""
	local CATEGORY=""
	while read CATEGORY ; do
		OLD="$(jq -er ".${CATEGORY}" <<< "${OLD_ARTIFACTS}")" || OLD=""
		NEW="$(jq -er ".${CATEGORY}" <<< "${NEW_ARTIFACTS}")" || NEW=""

		# If the artifact category no longer exists, then
		# we don't account for it as a changed category
		[ -n "${NEW}" ] || continue

		# This has changed (either didn't exist, or has a different
		# checksum), so we account for it as a changed category
		[ "${OLD}" != "${NEW}" ] && echo "${CATEGORY}"
	done < <(jq -r '.art[0] * .art[1] | keys[]' <<< "{ \"art\": [${OLD_ARTIFACTS}, ${NEW_ARTIFACTS}]}" | sort -u)
	return 0
}

list_changed_pods()
{
	local NAMESPACE="${1}"
	local RELEASE="${2}"
	local OLD_PODS="${3}"
	local NEW_PODS="${4}"
	shift 4

	local OLD=""
	local OLD_UID=""
	local NEW=""
	local NEW_UID=""
	for CATEGORY in "${@}" ; do

		# Compare each pod's UID from before and now to identify which
		# pods were rolled already, vs. which ones still need rolling
		while read POD ; do
			read OLD_UID OLD < <(grep -e " ${POD}$" <<< "${OLD_PODS}") || true
			read NEW_UID NEW < <(grep -e " ${POD}$" <<< "${NEW_PODS}") || true

			# If the pod existed, and there's no change in the UID, then
			# the pod needs to be rolled, so add it to the list!
			[ -n "${OLD_UID}" ] && [ "${OLD_UID}" == "${NEW_UID}" ] && echo -e "${POD}"
		done < <(kubectl --namespace "${NAMESPACE}" get pods --selector "${DEPLOY_LABEL}-${CATEGORY}=true" -o name | sed -e 's;^pod/;;g')
	done | sort -u | sed -e '/^\s*$/d'
	return 0
}

smart_rollover()
{
	local NAMESPACE="${1}"
	local RELEASE="${2}"
	local OLD_ARTIFACTS="${3}"
	local OLD_PODS="${4}"

	if [ -z "${DEPLOY_LABEL}" ] ; then
		warn "No smart rollover label is set (or it's blank) - cannot apply smart pod rollovers"
		return 0
	fi

	warn "Smart pod rollover will be computed using the smart rollover label ${DEPLOY_LABEL}"
	eyes "Old artifact checksum summaries:\n${OLD_ARTIFACTS}"

	local RC=0
	local NEW_ARTIFACTS=""
	NEW_ARTIFACTS="$(get_global_sums "${NAMESPACE}" "${RELEASE}" 2>&1)" || RC=${?}
	if [ ${RC} -ne 0 ] ; then
		err "Failed to obtain the updated global sums for the release [${RELEASE}] (rc=${RC}):\n${NEW_ARTIFACTS}"
		warn "Cannot process a smart rollover with the available information, must assume the deployment is complete"
		return 0
	fi

	# If we got the artifact sums, we can compare them!
	eyes "New artifact checksum summaries:\n${NEW_ARTIFACTS}"

	if [ "${OLD_ARTIFACTS}" == "${NEW_ARTIFACTS}" ] ; then
		ok "No changes in artifacts summaries! Will not roll over any additional pods"
		return 0
	fi

	# Get the list of changed categories
	local CATEGORIES=()
	readarray -t CATEGORIES < <(list_changed_categories "${OLD_ARTIFACTS}" "${NEW_ARTIFACTS}" | sed -e '/^\s*$/d')

	# If no categories changed, we do nothing because any pods that would
	# need to be rolled due to template changes would have been rolled already...
	if [ ${#CATEGORIES[@]} -eq 0 ] ; then
		ok "No changes in artifact categories! Will not roll over any additional pods"
		return 0
	fi

	# We have a list of changed categories, so roll their pods
	eyes "The following artifact categories have changed: [ ${CATEGORIES[@]} ]"

	eyes "Searching for pods that consume the changed artifact categories..."
	local NEW_PODS="$(list_consumer_uids "${NAMESPACE}" "${RELEASE}")"

	local ROLL=()
	readarray -t ROLL < <(list_changed_pods "${NAMESPACE}" "${RELEASE}" "${OLD_PODS}" "${NEW_PODS}" "${CATEGORIES[@]}")

	# rolled, we simply return
	if [ ${#ROLL[@]} -eq 0 ] ; then
		ok "All pods that needed to be rolled have already been rolled!"
		return 0
	fi

	# And ... roll!!
	doing "These pods need to be rolled due to deployment artifact changes: [ ${ROLL[@]} ]"
	if ! execute kubectl --namespace "${NAMESPACE}" delete pod "${ROLL[@]}" --wait --timeout 5m ; then
		err "Pod roll timed out - the deployment may end up unstable..."
		return 1
	fi

	# Now we wait ...
	waiting "Wait for the rolled pods to become ready again..."
	if ! execute kubectl --namespace "${NAMESPACE}" wait --for=condition=Ready pod "${ROLL[@]}" --timeout 5m ; then
		err "Failed to wait for the rolled pods to become initialized"
		return 1
	fi

	# All's well that ends well!
	ok "Rolled all pods dependent on the artifacts pod"
	return 0
}

sanitize_label()
{
	local LABEL="${1}"

	[[ "${LABEL}" =~ ^(([-a-zA-Z0-9.]{1,253})/)?(([a-zA-Z0-9][-a-zA-Z0-9_.]{0,45})?[a-zA-Z0-9])$ ]] || return 1

	local PREFIX="${BASH_REMATCH[2]}"
	local NAME="${BASH_REMATCH[3]}"

	is_valid_hostname "${PREFIX}" || return 2

	echo -n "${LABEL}"
	return 0
}

usage()
{
	echo -e "usage: ${BASH_SOURCE:-${0}} [namespace] release chart environment [--helm extraHelmArgs...]" 1>&2
	exit 1
}

BASE_ARGS=()
for a in "${@}" ; do
	case "${a}" in
		--helm ) shift ; break ;;
		* ) BASE_ARGS+=("${a}") ; shift ;;
	esac
done
HELM_ARGS=("${@}")

set -- "${BASE_ARGS[@]}"

[ ${#} -ge 3 ] && [ ${#} -le 4 ] || usage

if [ "${#}" -eq 3 ] ; then
	NAMESPACE="$(kubectl config view --minify -o jsonpath="{..namespace}")"
	[ -n "${NAMESPACE}" ] || NAMESPACE="default"
else
	NAMESPACE="${1}"
	shift
fi
is_valid_hostname_part "${NAMESPACE}" || fail "Invalid namespace name [${NAMESPACE}]"

#
# The name of the helm release to deploy
#
RELEASE="${1}"
is_valid_hostname_part "${RELEASE}" || fail "Invalid release name [${RELEASE}]"

#
# The name of the helm chart to deploy
#
CHART="${2}"
[ -n "${CHART}" ] || fail "The chart name may not be the empty string"

#
# Ensure we got the right environment n ame
#
ENVIRONMENT_NAME="${3}"
is_valid_hostname_part "${ENVIRONMENT_NAME}" || fail "Invalid environment name [${ENVIRONMENT_NAME}]"

[ -v DEPLOYMENT_DIR ] || DEPLOYMENT_DIR=""
[ -n "${DEPLOYMENT_DIR}" ] || DEPLOYMENT_DIR="${PWD}"
DEPLOYMENT_DIR="$(readlink -f "${DEPLOYMENT_DIR}")" || fail "Failed to canonicalize the path for [${DEPLOYMENT_DIR}]"

say "Deploying from [${DEPLOYMENT_DIR}]..."
[ -f "${DEPLOYMENT_DIR}/deployment.yaml" ] || fail "This does not seem to be a deployment directory - no deployment.yaml was found!"

#
# This will hold the list of Helm "-f" parameters to include
# every single values file that needs to be taken into account
# for this deployment
#
VALUES=()

#
# Step one: find the common files
#
#
# The directory where the common files (i.e. files shared across
# directories) are stored.
#
[ -v COMMON_DIRS ] || COMMON_DIRS="common"
readarray -t COMMON_DIRS < <(sanitize_common_dirs "${COMMON_DIRS}")
for D in "${COMMON_DIRS[@]}" ; do
	COMMON_DIR="${DEPLOYMENT_DIR}/${D}"
	if [ -d "${COMMON_DIR}" ] ; then
		while read V ; do
			VALUES+=(-f "${V}")
		done < <(find "${COMMON_DIR}" -type f -iname '*.yaml' | sort -u)
	else
		warn "No common values are being applied from [${COMMON_DIR}] - the directory doesn't exist"
	fi
done

#
# Step two: find the files in the environment's directory
#
#
# The root directory where the environment directories
# are housed. The directory name must match the name
# of the environment being deployed *EXACTLY*
#
ENV_DIR="${DEPLOYMENT_DIR}/envs/${ENVIRONMENT_NAME}"

if [ -d "${ENV_DIR}" ] ; then
	while read V ; do
		VALUES+=(-f "${V}")
	done < <(find "${ENV_DIR}" -type f -iname '*.yaml' | sort -u)
else
	warn "No environment folder found at [${DEPLOYMENT_DIR}] for [${ENVIRONMENT_NAME}]"
fi

ensure_namespace_exists "${NAMESPACE}" || fail "Failed to ensure that the required namespace ${NAMESPACE} exists"

set -- "${HELM_ARGS[@]}"

if as_boolean "${DISABLE_UPGRADE}" ; then
	err "In-place upgrade deployment is disabled, will uninstall and reinstall"
else
	if [ -n "${DEPLOY_LABEL}" ] ; then
		eyes "Validating the smart rollover label [${DEPLOY_LABEL}]..."
		RC=0
		DEPLOY_LABEL="$(sanitize_label "${DEPLOY_LABEL}")" || RC=${?}
		case "${RC}" in
			0 ) ok "The smart rollover label is valid!" ;;
			1 ) err "The smart rollover label's syntax is not valid, ignoring it" ; DEPLOY_LABEL="" ;;
			2 ) err "The smart rollover label's prefix is not valid RFC-1123 hostname, ignoring it" ; DEPLOY_LABEL="" ;;
		esac
	fi

	RC=0
	OLD_ARTIFACTS="$(get_global_sums "${NAMESPACE}" "${RELEASE}" 2>&1)" || RC=${?}
	if [ ${RC} -ne 0 ] ; then
		warn "Failed to fetch the old artifact checksums (rc=${RC}):\n${OLD_ARTIFACTS}"
		OLD_ARTIFACTS="{}"
	fi

	OLD_PODS="$(list_consumer_uids "${NAMESPACE}" "${RELEASE}")"
	OLD_ARTIFACT_PODS="$(list_artifact_uids "${NAMESPACE}" "${RELEASE}")"

	#
	# Compute how many history items to keep
	#
	if ! [[ "${HISTORY_MAX}" =~ ^[1-9][0-9]*$ ]] ; then
		case "${HISTORY_MAX,,}" in
			"0" | "all" ) HISTORY_MAX="0" ;;
			* ) HISTORY_MAX="" ;;
		esac
	fi

	# Set the --history-max parameter
	HISTORY_ARG=()
	[ -n "${HISTORY_MAX}" ] && HISTORY_ARG=(--history-max "${HISTORY_MAX}")

	#
	# First, try an in-place upgrade. If that fails, then undeploy and redeploy.
	#
	# Since we're using --wait, this will not exit until all the pods are ready,
	# which allows us to analyze them safely and make a robust decision about
	# which pods actually needed to be rolled, but weren't
	#
	RC=0
	execute \
		helm upgrade --install \
			"${RELEASE}" "${CHART}" \
			--namespace "${NAMESPACE}" \
			"${HISTORY_ARG[@]}" \
			--wait \
			--timeout "${DEPLOY_TIMEOUT}" \
			"${VALUES[@]}" \
			"${@}" || RC=${?}

	if [ ${RC} -eq 0 ] ; then

		roll_artifact_pods "${NAMESPACE}" "${RELEASE}" "${OLD_ARTIFACT_PODS}" || \
			warn "The artifacts pods couldn't be rolled - smart rollover may not work as intended"

		ok "Deployment complete! Attempting the conditional rollover"
		smart_rollover "${NAMESPACE}" "${RELEASE}" "${OLD_ARTIFACTS}" "${OLD_PODS}" || RC=${?}
		if [ ${RC} -eq 0 ] ; then
			ok "Conditional rollover seems to have worked!"
			execute kubectl --namespace "${NAMESPACE}" get pods
			# Not needed, but just in case ...
			exit 0
		fi

		# Uh-oh ... Danger, Will Robinson!
		err "Conditional rollover appears to have failed (rc=${RC})"
	fi

	err "In-place upgrade attempt failed (rc=${RC}), will try to uninstall and reinstall"
fi

# The in-place upgrade failed ... we need to fully uninstall and
# then re-install the application!
#
# Propagate the "disable" flag, to do nothing if we're testing
DISABLE_UNDEPLOY="${DISABLE_DEPLOY}" "${BASEDIR}/undeploy-environment" "${NAMESPACE}" "${RELEASE}" || exit ${?}

# Execute the new installation, using install --replace
# because we intend to be preserving history from our
# uninstalls
RC=0
execute \
	helm install --replace \
		"${RELEASE}" "${CHART}" \
		--namespace "${NAMESPACE}" \
		--wait \
		--timeout "${DEPLOY_TIMEOUT}" \
		"${VALUES[@]}" \
		"${@}" || RC=${?}

if [ ${RC} -eq 0 ] ; then
	ok "Deployment complete!"
else
	# Something went wrong, try to offer up more information
	err "The Helm deployment has failed (rc=${RC}), please review the logs"
fi

execute kubectl --namespace "${NAMESPACE}" get pods
exit ${RC}
