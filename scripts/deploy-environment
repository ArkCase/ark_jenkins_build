#!/bin/bash
SCRIPT="$(readlink -f "${BASH_SOURCE:-${0}}")"
BASEDIR="$(dirname "${SCRIPT}")"

set -euo pipefail
. /.functions

set_or_default DEPLOY_TIMEOUT "20m"

set_as_boolean DISABLE_DEPLOY
export DISABLE_DEPLOY

set_as_boolean PARTIAL_DEPLOYMENT
export PARTIAL_DEPLOYMENT

execute()
{
	execute_unless DISABLE_DEPLOY "${@}"
	return ${?}
}

ensure_namespace_exists()
{
	local NAMESPACE="${1}"
	execute kubectl get namespace "${NAMESPACE}" &>/dev/null && return 0
	execute kubectl create namespace "${NAMESPACE}" && return 0
	return 1
}

sanitize_paths()
{
	local STR="${1}"
	local SEP=":"
	local PARTS=()

	# First, split into parts so we can retain the
	# declaration order
	readarray -t PARTS < <(tr "${SEP}" "\n" <<< "${STR}" | sed -e '/^\s*$/d')
	local LIST=":"
	for P in "${PARTS[@]}" ; do
		[ -n "${P}" ] || continue
		# Skip it if already marked
		fgrep -q ":${P}:" <<< "${LIST}" && continue
		LIST+="${P}:"
		echo "${P}"
	done
}

sanitize_common_dirs()
{
	local COMMON_DIRS=()
	# We split this in two so it's easy to lift the sanitize_paths function for other purposes
	readarray -t COMMON_DIRS < <(sanitize_paths "${1}")
	for C in "${COMMON_DIRS[@]}" ; do
		[[ "${C}" =~ ^(/*)(.*)$ ]] && C="${BASH_REMATCH[2]}"
		if [[ "${C}" =~ (^|/)[.]{1,2}(/|$) ]] ; then
			warn "Skipping the common directory specification [${C}]: may not contain '.' or '..' as path components"
			continue
		fi
		echo "${C}"
	done
}

get_artifacts_selector()
{
	local RELEASE="${1}"
	echo -n "app.kubernetes.io/instance=${RELEASE},app.kubernetes.io/name=app,app.kubernetes.io/part=artifacts"
}

get_artifacts_image()
{
	local RELEASE="${1}"
	local JQ_EXP='.items[] | .status.containerStatuses[] | select(.name == "artifacts") | .imageID'
	kubectl get pod -l "$(get_artifacts_selector "${RELEASE}")"  -o json | jq -r "${JQ_EXP}" | sort -u
}

roll_pods_if_necessary()
{
	local RELEASE="${1}"
	local OLD="${2}"

	# Roll the artifacts pod, and wait for it to be ready again...
	local SELECTOR="$(get_artifacts_selector "${RELEASE}")"

	# Roll the pod
	if execute kubectl delete pod -l "${SELECTOR}" ; then
		err "Failed to roll the artifacts pod (selector = [${SELECTOR}])"
		return 0
	fi

	# Wait for it to be ready
	if ! kubectl wait --for=condition=Initialized pod -l "${SELECTOR}" --timeout 5m ; then
		err "The artifacts pod failed to be initialized in the allowed timeframe - can't check the image"
		return 0
	fi

	# Get the new image
	local NEW="$(get_artifacts_image "${RELEASE}")"

	# If the new one is blank, we don't need to roll anything
	[ -n "${NEW}" ] || return 0

	# If they're the same, there's no need to roll anything
	[ "${OLD}" != "${NEW}" ] || return 0

	# If they're different, then def we should roll whatever needs rolling

	# TODO: Shall we check each dependent pod in turn to see if rolling is needed? How?

	SELECTOR="app.kubernetes.io/instance=${RELEASE},arkcase.com/pod-depends=artifacts"
	if ! execute kubectl delete pod -l "${SELECTOR}" ; then
		err "Failed to roll the dependent pods (selector = [${SELECTOR}])"
		return 0
	fi

	if ! kubectl wait --for=condition=Ready pod -l "${SELECTOR}" --timeout "${DEPLOY_TIMEOUT}" ; then
		err "The artifact-dependent pods did not become ready after the stipulated timeout (${DEPLOY_TIMEOUT}, selector = [${SELECTOR}])"
		return 0
	fi

	ok "Rolled all pods dependent on the artifacts pod"
	return 0
}

usage()
{
	echo -e "usage: ${BASH_SOURCE:-${0}} [namespace] release chart environment [--helm extraHelmArgs...]" 1>&2
	exit 1
}

BASE_ARGS=()
for a in "${@}" ; do
	case "${a}" in
		--helm ) shift ; break ;;
		* ) BASE_ARGS+=("${a}") ; shift ;;
	esac
done
HELM_ARGS=("${@}")

set -- "${BASE_ARGS[@]}"

[ ${#} -ge 3 ] && [ ${#} -le 4 ] || usage

if [ "${#}" -eq 3 ] ; then
	NAMESPACE="$(kubectl config view --minify -o jsonpath="{..namespace}")"
	[ -n "${NAMESPACE}" ] || NAMESPACE="default"
else
	NAMESPACE="${1}"
	shift
fi
is_valid_hostname_part "${NAMESPACE}" || fail "Invalid namespace name [${NAMESPACE}]"

#
# The name of the helm release to deploy
#
RELEASE="${1}"
is_valid_hostname_part "${RELEASE}" || fail "Invalid release name [${RELEASE}]"

#
# The name of the helm chart to deploy
#
CHART="${2}"
[ -n "${CHART}" ] || fail "The chart name may not be the empty string"

#
# Ensure we got the right environment n ame
#
ENVIRONMENT_NAME="${3}"
is_valid_hostname_part "${ENVIRONMENT_NAME}" || fail "Invalid environment name [${ENVIRONMENT_NAME}]"

[ -v DEPLOYMENT_DIR ] || DEPLOYMENT_DIR=""
[ -n "${DEPLOYMENT_DIR}" ] || DEPLOYMENT_DIR="${PWD}"
DEPLOYMENT_DIR="$(readlink -f "${DEPLOYMENT_DIR}")" || fail "Failed to canonicalize the path for [${DEPLOYMENT_DIR}]"

say "Deploying from [${DEPLOYMENT_DIR}]..."
[ -f "${DEPLOYMENT_DIR}/deployment.yaml" ] || fail "This does not seem to be a deployment directory - no deployment.yaml was found!"

#
# This will hold the list of Helm "-f" parameters to include
# every single values file that needs to be taken into account
# for this deployment
#
VALUES=()

#
# Step one: find the common files
#
#
# The directory where the common files (i.e. files shared across
# directories) are stored.
#
[ -v COMMON_DIRS ] || COMMON_DIRS="common"
readarray -t COMMON_DIRS < <(sanitize_common_dirs "${COMMON_DIRS}")
for D in "${COMMON_DIRS[@]}" ; do
	COMMON_DIR="${DEPLOYMENT_DIR}/${D}"
	if [ -d "${COMMON_DIR}" ] ; then
		while read V ; do
			VALUES+=(-f "${V}")
		done < <(find "${COMMON_DIR}" -type f -iname '*.yaml' | sort -u)
	else
		warn "No common values are being applied from [${COMMON_DIR}] - the directory doesn't exist"
	fi
done

#
# Step two: find the files in the environment's directory
#
#
# The root directory where the environment directories
# are housed. The directory name must match the name
# of the environment being deployed *EXACTLY*
#
ENV_DIR="${DEPLOYMENT_DIR}/envs/${ENVIRONMENT_NAME}"

if [ -d "${ENV_DIR}" ] ; then
	while read V ; do
		VALUES+=(-f "${V}")
	done < <(find "${ENV_DIR}" -type f -iname '*.yaml' | sort -u)
else
	warn "No environment folder found at [${DEPLOYMENT_DIR}] for [${ENVIRONMENT_NAME}]"
fi

ensure_namespace_exists "${NAMESPACE}" || fail "Failed to ensure that the required namespace ${NAMESPACE} exists"

set -- "${HELM_ARGS[@]}"

#
# We use this to gate partial deployments (i.e. in-place upgrades)
# b/c we haven't yet fully figured out how to properly handle those.
#
if "${PARTIAL_DEPLOYMENT}" ; then

	# Take note of the container image that's currently in use by the
	# artifacts pod, so we can determine if dependent pods need to be
	# rolled later
	OLD_ARTIFACTS="$(get_artifacts_image "${RELEASE}")"

	#
	# TODO: one possible way to identify pods that need to be rolled is by
	# comparing their "Initialized" timestamp to the artifacts pod's
	# "Ready" timestamp. If they were initialized before the new artifacts
	# pod became ready, then clearly they will have to be rolled so they
	# can consume the new artifacts.
	#

	#
	# First, try an in-place upgrade. If that fails, then undeploy and redeploy
	#
	RC=0
	execute \
		helm upgrade --install \
			"${RELEASE}" "${CHART}" \
			--namespace "${NAMESPACE}" \
			--wait \
			--timeout "${DEPLOY_TIMEOUT}" \
			"${VALUES[@]}" \
			"${@}" || RC=${?}

	if [ ${RC} -eq 0 ] ; then
		ok "Deployment complete!"
		roll_pods_if_necessary "${RELEASE}" "${OLD_ARTIFACTS}"
		execute kubectl --namespace "${NAMESPACE}" get pods
		exit ${RC}
	fi

	err "In-place upgrade failed, will try to uninstall and reinstall"
fi

# The in-place upgrade failed ... we need to fully uninstall and
# then re-install the application!
#
# Propagate the "disable" flag, to do nothing if we're testing
DISABLE_UNDEPLOY="${DISABLE_DEPLOY}" "${BASEDIR}/undeploy-environment" "${NAMESPACE}" "${RELEASE}" || exit ${?}

# Execute the new installation, using install --replace
# because we intend to be preserving history from our
# uninstalls
RC=0
execute \
	helm install --replace \
		"${RELEASE}" "${CHART}" \
		--namespace "${NAMESPACE}" \
		--wait \
		--timeout "${DEPLOY_TIMEOUT}" \
		"${VALUES[@]}" \
		"${@}" || RC=${?}

if [ ${RC} -eq 0 ] ; then
	ok "Deployment complete!"
else
	# Something went wrong, try to offer up more information
	err "The Helm deployment has failed, please review the logs"
fi

execute kubectl --namespace "${NAMESPACE}" get pods
exit ${RC}
